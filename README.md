# LOLA â€” Local Ollama Language Assistant

**LOLA** is a fast, private, terminal-based AI chat assistant powered by [Ollama](https://ollama.com).  
No API keys. No cloud. No fees. Everything runs locally on your machine.

> ğŸ’¡ **Why LOLA?** Running AI locally means your conversations never leave your computer,
> there are no usage limits, and no monthly bills. The only cost is electricity.

---

## âœ¨ Features

- ğŸ–¥ï¸ **Cross-Platform** â€” Works on Linux (X11/Wayland) and macOS.
- ğŸ¨ **Rich terminal UI** â€” `figlet` ASCII banner, `gum`-styled borders, animated spinner
- ğŸ–¥ï¸ **Unified inline menus** â€” `gum filter` fuzzy search, stays inside the
terminal on both X11 and Wayland
- ğŸ“‹ **Auto clipboard** â€” every response is copied automatically (`wl-copy` / `xsel`)
- ğŸ’¾ **Session management** â€” save, load, edit, and remove named chat sessions
- ğŸ”„ **Live model switching** â€” swap models mid-conversation with `!switch`
- ğŸ•µï¸ **Agent personas** â€” switch system prompts (Coder, Writer, Teacherâ€¦) with `!agent`
- ğŸ“… **Honesty protocol** â€” current date injected; model instructed not to hallucinate
- ğŸ–¼ï¸ **Vision analysis** â€” analyze images via a local vision model with `!vision`
- ğŸŒ **Web search** â€” launch a background web search with `!web`
- ğŸ“ **Multi-line input** â€” paste a block of text and press `Ctrl+D` to submit
- ğŸ—‚ï¸ **Modular codebase** â€” clean `lib/` structure, easy to extend

---

## ğŸ–¥ï¸ Choosing a Model for Your Hardware

LOLA runs 100% locally â€” the right model depends on your GPU VRAM or system RAM.

| VRAM / RAM   | Recommended Model        | Notes                              |
|--------------|--------------------------|------------------------------------|
| CPU only     | `ministral-3b`           | Slow but works without a GPU       |
| < 4 GB       | `ministral-3b`           | Fast, minimal footprint            |
| 4â€“6 GB       | `llama3.2:latest`        | Great general-purpose model        |
| 6â€“8 GB       | `qwen2.5-coder:7b`       | Best for coding tasks              |
| 8â€“12 GB      | `qwen3:4b` / `mistral`   | Balanced speed + quality           |
| 12 GB+       | `llama3.1:8b` and above  | Near-GPT-4 quality, fully local    |

> ğŸ’¡ Check your VRAM: `nvidia-smi` (NVIDIA) Â· `rocm-smi` (AMD) Â· `intel_gpu_top` (Intel)  
> ğŸ’¡ Check your RAM: `free -h`

## Local models I use based on my hardware:
NAME                     ID              SIZE      MODIFIED    
devstral-small-2:24b     24277f07f62d    15 GB     12 days ago    
ministral-3:8b           1922accd5827    6.0 GB    12 days ago    
qwen3:4b                 359d7dd4bcda    2.5 GB    12 days ago    
granite3.2-vision:2b     3be41a661804    2.4 GB    2 weeks ago    
translategemma:latest    c49d986b0764    3.3 GB    2 weeks ago    
llama3.2:latest          a80c4f17acd5    2.0 GB    7 weeks ago    

---

## ğŸ“¦ Installation

### 1. Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. Pull a model (choose based on your hardware above)

```bash
ollama pull llama3.2:latest       # 4â€“6 GB VRAM
ollama pull qwen2.5-coder:7b      # 6â€“8 GB VRAM (great for coding)
ollama pull ministral-3b          # CPU / low VRAM
```

### 3. Install script dependencies

```bash
# macOS (using Homebrew)
brew install figlet fzf jq curl neovim gum

# Arch Linux
sudo pacman -S figlet fzf jq curl neovim xsel gum

# Debian / Ubuntu
sudo apt install figlet fzf jq curl neovim xsel
# gum is installed manually, see https://github.com/charmbracelet/gum#installation
```

### 4. Clone and run

```bash
git clone https://github.com/nisidabay/lola.git ~/bin/ollama_chat
chmod +x ~/bin/ollama_chat/lola.sh
~/bin/ollama_chat/lola.sh
```

---

## ğŸš€ Usage

```bash
./lola.sh
```

You'll see the **LOLA** ASCII banner, the active model, and a `â¯` prompt.  
Type your question and press **Enter**. An animated spinner appears while the model thinks,  
then the response is printed and copied to your clipboard automatically.

Type `exit` or `quit` to leave cleanly.

---

## âŒ¨ï¸ Commands

| Command                | Description                                          |
|------------------------|------------------------------------------------------|
| `!menu` / `!m`         | Show the help menu                                   |
| `!clear`               | Clear chat history                                   |
| `!history` / `!his`    | View full conversation history                       |
| `!last`                | Copy last AI response to clipboard                   |
| `!new_chat` / `!new`   | Start a new chat (clears history)                    |
| `!save` / `!sa`        | Save current chat to disk                            |
| `!load` / `!lo`        | Load a saved chat session                            |
| `!rm`                  | Remove a saved chat                                  |
| `!edit_saved` / `!es`  | Edit a saved chat in your pager                      |
| `!switch` / `!sw`      | Switch AI model on the fly                           |
| `!web`                 | Launch a background web search                       |
| `!vision` / `!img`     | Analyze an image (JPG/PNG) with the vision model     |
| `!agent` / `!a`        | Switch agent persona                                 |
| `!terminal` / `!t`     | Open a new detached terminal                         |
| `!kill` / `!k`         | Stop Ollama server and exit                          |
| `exit` / `quit`        | Quit the script                                      |

---

## ğŸ—‚ï¸ Project Structure

```
ollama_chat/
â”œâ”€â”€ lola.sh       # Entry point (~140 lines)
â”œâ”€â”€ web_search.sh           # Web search helper
â””â”€â”€ lib/
    â”œâ”€â”€ ui.sh               # Banner, separators, styled output, help menu
    â”œâ”€â”€ chat.sh             # Main chat loop, history, clear, last
    â”œâ”€â”€ session.sh          # Save, load, remove, edit chat sessions
    â”œâ”€â”€ models.sh           # Model selection, agent switching, server restart
    â””â”€â”€ helpers.sh          # Web search, terminal launcher, vision analysis
```

LOLA seamlessly stores your active configurations and sessions via the XDG Base Directory specification:
- **Configuration** (`lola.conf`): `~/.config/lola/lola.conf`
- **Session DB & History**: `~/.cache/lola/`

---

## âš™ï¸ Configuration

Edit `~/.config/lola/lola.conf` to customize, though MODEL is updated with the `!switch` | `!sw` commands:

```conf
MODEL="qwen3:4b"
VISION_MODEL="granite3.2-vision:2b"
EDITOR=nvim
PAGER=nvim

# Browser for web_search.sh (change to: chromium, brave, xdg-open, etc.)
BROWSER="firefox"

# Search engines for web_search.sh
declare -A SEARCH_ENGINES_CONF
SEARCH_ENGINES_CONF[brave]="https://search.brave.com/search?q="
SEARCH_ENGINES_CONF[duck]="https://duckduckgo.com/?q="

# Agent system prompts
declare -A AGENTS_CONF
AGENTS_CONF[default]="You are a helpful assistant."
AGENTS_CONF[coder]="You are an expert software engineer."
AGENTS_CONF[writer]="You are a creative writer."
AGENTS_CONF[teacher]="You are a patient teacher."
AGENTS_CONF[concise]="Be extremely concise. Give only the answer, no filler."
```

---

## ğŸ”§ Dependencies

| Tool       | Purpose                                  | Required      |
|------------|------------------------------------------|---------------|
| `ollama`   | Local LLM runtime                        | âœ… Yes        |
| `gum`      | Styled UI (spinner, menus, borders)      | âœ… Yes        |
| `figlet`   | ASCII banner                             | âœ… Yes        |
| `fzf`      | Image file picker for `!vision`          | âœ… Yes        |
| `jq`       | JSON parsing for vision API              | âœ… Yes        |
| `curl`     | Vision model API calls                   | âœ… Yes        |
| `nvim`     | Default pager/editor                     | âœ… Yes        |
| `pbcopy`   | Clipboard on macOS                       | macOS only    |
| `wl-copy`  | Clipboard on Wayland                     | Wayland only  |
| `foot`     | Terminal launcher on Wayland             | Wayland only  |
| `xsel`     | Clipboard on X11                         | X11 only      |
| `st`       | Terminal launcher on X11                 | X11 only      |

---

## ğŸ› ï¸ Troubleshooting

| Problem                        | Fix                                              |
|--------------------------------|--------------------------------------------------|
| No models in `ollama list`     | Run `ollama pull llama3.2:latest`                |
| Model too slow                 | Switch to a smaller model (see hardware table)   |
| Clipboard not working          | Check `xsel` (X11), `wl-copy` (Wayland), or `pbcopy` (macOS) |
| Script exits prematurely       | Check Ollama is running: `pgrep ollama`          |
| Vision model not found         | Run `ollama pull <VISION_MODEL>`                 |
| Spinner flickers at start      | Ensure `gum` â‰¥ 0.14 is installed                |

---

## ğŸ“ Notes

- In **tmux**, the prompt turns red and a warning is shown. Use **Ctrl+C** to quit.
- All chat logs are stored locally (`~/.cache/lola/`) â€” no internet required for chat.
- When switching models, the previous one is stopped cleanly via `ollama stop`.
- The script prompts for a model via `gum filter` if none is set in the config.

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE).
